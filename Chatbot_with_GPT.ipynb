{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingmoe21/tech_innovators/blob/main/Chatbot_with_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain constants chromadb tiktoken datasets transformers imagededup Pillow matplotlib-venn cartopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcrimgEuX3Ba",
        "outputId": "77daceb4-9265-488a-d730-c7e926528b7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.4-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.335-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting constants\n",
            "  Downloading constants-2023.2.0.tar.gz (5.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.17-py3-none-any.whl (496 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.8/496.8 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imagededup\n",
            "  Downloading imagededup-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.10/dist-packages (0.11.9)\n",
            "Collecting cartopy\n",
            "  Downloading Cartopy-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.64-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting tox (from constants)\n",
            "  Downloading tox-4.11.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of constants to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting constants\n",
            "  Downloading constants-0.6.0.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.19.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from imagededup) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from imagededup) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from imagededup) (1.2.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagededup) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imagededup) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.11.3)\n",
            "Requirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.10/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: pyshp>=2.1 in /usr/local/lib/python3.10/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from cartopy) (3.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.5 (from openai)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagededup) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagededup) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagededup) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagededup) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imagededup) (3.1.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->imagededup) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->imagededup) (3.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->imagededup) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->imagededup) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->imagededup) (2.1.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->imagededup) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Building wheels for collected packages: constants, pypika\n",
            "  Building wheel for constants (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for constants: filename=constants-0.6.0-py3-none-any.whl size=5457 sha256=8dabd3b5d975e16027d89d8b4fe74eb5456ededfb3e3c94c18656114ae899a9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/96/3c/386c2342a8a1bdd317f2f250bd076c13938c6f598c4a40ec14\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=fc2ed5c115c4aee1cda7259fe9bd62ba53d0e3d729fd7361d20f33485a7ba42b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built constants pypika\n",
            "Installing collected packages: pypika, monotonic, constants, websockets, uvloop, urllib3, typing-extensions, safetensors, python-dotenv, pyarrow-hotfix, pulsar-client, overrides, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, humanfriendly, httptools, h11, dill, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, multiprocess, jsonpatch, httpcore, coloredlogs, tiktoken, posthog, opentelemetry-sdk, onnxruntime, langsmith, huggingface-hub, httpx, fastapi, dataclasses-json, cartopy, tokenizers, opentelemetry-exporter-otlp-proto-grpc, openai, langchain, kubernetes, imagededup, datasets, transformers, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.0.1 cartopy-0.22.0 chroma-hnswlib-0.7.3 chromadb-0.4.17 coloredlogs-15.0.1 constants-0.6.0 dataclasses-json-0.6.2 datasets-2.14.7 deprecated-1.2.14 dill-0.3.7 fastapi-0.104.1 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.25.1 huggingface-hub-0.17.3 humanfriendly-10.0 imagededup-0.3.2 jsonpatch-1.33 jsonpointer-2.4 kubernetes-28.1.0 langchain-0.0.335 langsmith-0.0.64 marshmallow-3.20.1 monotonic-1.6 multiprocess-0.70.15 mypy-extensions-1.0.0 onnxruntime-1.16.2 openai-1.2.4 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pyarrow-hotfix-0.5 pypika-0.48.9 python-dotenv-1.0.0 safetensors-0.4.0 starlette-0.27.0 tiktoken-0.5.1 tokenizers-0.14.1 transformers-4.35.1 typing-extensions-4.8.0 typing-inspect-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "x4LBG8x5UOdy",
        "outputId": "9a3c9494-67f5-4a69-aabb-67b015a96289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.2.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "feaFw5SiHJcQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Activation\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.metrics  import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from  tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import glob as gb\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "import sys\n",
        "import cartopy\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import Chroma\n",
        "import constants\n",
        "from openai import OpenAI\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/Recording3.txt\"  # قم بتغيير \"اسم الملف.txt\" إلى اسم الملف الذي قمت بتحميله\n",
        "\n",
        "with open(file_name, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1jd68zcXD-3",
        "outputId": "28f38cfa-40d5-468e-ec92-8b44f27f8ba1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.60 - 1.81]: So what, um...\n",
            "[2.42 - 6.81]: So we'll start with the end bit of a game.\n",
            "[7.02 - 9.62]: There we go, hopefully most of you.\n",
            "[9.68 - 13.27]: Recognize that war let's it's out fairly soon for my friends at Sony\n",
            "[13.30 - 13.80]: Thank you.\n",
            "[13.97 - 15.02]: Thank you.\n",
            "[15.06 - 15.74]: Thank you.\n",
            "[16.22 - 18.09]: Um...\n",
            "[18.19 - 20.50]: That's an image from a game.\n",
            "[20.52 - 26.54]: To be honest, that's probably a mock-up. I suspect that's not a real screen grab, but it's increasingly\n",
            "[26.56 - 28.98]: difficult to tell with the latest hardware.\n",
            "[28.99 - 31.24]: It's a pretty nice image either way.\n",
            "[31.55 - 40.38]: So what are we looking at there? What is an image from a game? What does it consist of?\n",
            "[41.58 - 57.75]: What are the key, the building blocks of it if you like? Yeah? Models. Models, yeah, okay. What are models made of? Polygons. Polygons, there we go. Yeah, so everything in there. 3D models made out of 2D polygons put together.\n",
            "[57.76 - 59.45]: in a 3D world.\n",
            "[59.90 - 63.64]: But that's not what we can see there, is it? That's a 2D image.\n",
            "[63.75 - 65.37]: So what's the image made of?\n",
            "[65.39 - 66.10]: Thank you. Thank you.\n",
            "[66.46 - 66.93]: Thank you.\n",
            "[67.58 - 68.00]: Yeah.\n",
            "[68.01 - 69.99]: Pixels, there we go.\n",
            "[70.03 - 70.62]: Thank you.\n",
            "[70.74 - 71.09]: Thank you.\n",
            "[71.10 - 73.18]: pixels at every\n",
            "[73.25 - 85.52]: every spot if you like on a screen has an RGB value and that's effectively what we're looking at. When you're sat at home playing World of War, when that comes out, you've got a...\n",
            "[85.54 - 87.61]: to the telly\n",
            "[87.68 - 95.15]: displaying a 2D image which is built from these 3D models which have been animated, put into the world.\n",
            "[95.39 - 95.99]: time.\n",
            "[96.07 - 104.96]: Hopefully highly enjoyable games or any boring games. There's plenty of them around as well And this course this module I should say\n",
            "[105.12 - 108.34]: Really the decay of it, the decay of the...\n",
            "[108.78 - 113.51]: of the lectures and the exam really is how\n",
            "[113.55 - 121.14]: do we turn those 3D models created by artists\n",
            "[121.18 - 122.94]: into that 2D.\n",
            "[123.12 - 123.78]: image.\n",
            "[123.84 - 134.54]: which is moving, it's in real time, has to react, you press jump, you want to get the jump on the screen. How does that happen in real time? What algorithms do we use?\n",
            "[134.59 - 139.07]: to make that happen. And that's really what the core of this module is.\n",
            "[139.20 - 140.60]: is all about.\n",
            "[141.26 - 148.59]: We'll give you some practice in doing that. We'll be using Unity, which is an industry standard engine, to do that.\n",
            "[148.70 - 155.80]: But all the lectures are about, they're not really about the, well they're not at all about the implementation.\n",
            "[155.81 - 158.08]: in unity about what's going on.\n",
            "[158.14 - 165.19]: on the hardware. Why is the unit doing the thing it does in order to get those screens running as fast as possible?\n",
            "[166.40 - 172.47]: So just I guess a reminder, I'm sure you are reasonably aware of this.\n",
            "[172.86 - 181.58]: a Pixar movie or Avengers Endgame or whatever it is, if there's all this computer-generated imagery.\n",
            "[181.73 - 190.29]: in all his fantastic movies these days. Pixar famously devoted 36 hours to rendering each and every frame.\n",
            "[190.38 - 197.86]: What I mean by rendering a frame, so we've got those 3D models in the scene, we're going to go through that process of turning...\n",
            "[197.92 - 198.70]: each.\n",
            "[198.78 - 200.72]: each fragment to the image.\n",
            "[200.75 - 208.51]: The big thing that they spend the time on is getting the lighting right. There's an awful lot of work who isn't delighting. So you have your models, you can color them in.\n",
            "[208.75 - 214.78]: how the lid within the scene is well most of the time takes place. And that's really why...\n",
            "[214.82 - 216.86]: whatever the latest Pixar movie.\n",
            "[216.86 - 222.14]: is look so much better than whatever, well, the Cold War, Iraq for example.\n",
            "[222.62 - 224.94]: So, next time, sorry.\n",
            "[224.98 - 230.06]: Pixar Famously spends 36 hours on every single frame of their movies.\n",
            "[230.12 - 244.37]: They do them in parallel, they don't just do one and then 36 hours later they move on to the next one. They do lots of them in parallel, but every frame gets 36 hours of processing time, effectively turning those 3D models into a 2D image for the screen.\n",
            "[244.56 - 244.92]: Thank you.\n",
            "[244.95 - 245.44]: Thank you.\n",
            "[245.50 - 246.87]: Come on, do that in games.\n",
            "[246.91 - 253.73]: We've got 33 milliseconds to do exactly the same thing as Pixar have 36 hours to do.\n",
            "[255.45 - 255.83]: Thank you.\n",
            "[256.10 - 258.23]: So we've got it pretty efficient there really.\n",
            "[258.36 - 258.70]: Thank you.\n",
            "[258.86 - 259.26]: Thank you.\n",
            "[259.54 - 262.38]: Why do we only have 33 milliseconds?\n",
            "[263.20 - 263.89]: Thank you. Thank you.\n",
            "[264.59 - 266.08]:\n",
            "\n",
            "[266.18 - 266.62]: you\n",
            "[266.82 - 269.75]: Get 60 frames a second.\n",
            "[269.86 - 275.61]: OK, but why can't we do lots of them in parallel, so spend a minute on each one? Is this going to change?\n",
            "[275.75 - 285.27]: It's gonna change has to react what the players doing you press jump you want Mario to jump You don't want him going off and coming back 36 hours later saying yeah, I've tuned that\n",
            "[285.65 - 291.82]: So, it's really similar techniques used with Pixar as with Nintendo.\n",
            "[292.00 - 294.02]: But the key here is to get.\n",
            "[294.18 - 314.02]: Our rendering process is working as fast as possible while still looking amazing. You shouldn't be cutting corners just to get speed. It has to still look great. Hopefully one of the things we get across during this lecture series is that balancing act. You can always chuck in more fidelity, make it look greater. That's going to bring the frame rate down.\n",
            "[314.40 - 317.85]: So you've got to find ways of maintaining the visual quality.\n",
            "[318.14 - 321.12]: and also the frame rate.\n",
            "[321.26 - 327.67]: You play some of these big open world games, you go somewhere, drops down to 20 frames per second, you go, no, like that.\n",
            "[327.70 - 343.30]: And if you're in virtual reality, really you've got to be 120 frames per second, because you want this eye to be going at 60, and this eye to be going at 60. You end up in the same scene twice. So you've got to be even faster on virtual reality applications.\n",
            "[343.30 - 344.42]: Thank you.\n",
            "[344.53 - 345.22]: Thank you.\n",
            "[345.38 - 347.55]: Right, I've forgotten what's next.\n",
            "[347.60 - 364.31]: Yes, so, as I say, the main thing we'll be talking about through the lecture series is how do we do that in real time? How do we turn those 3D models into a 2D image at a rate of 60 frames per second? What's going on on the hardware? We're going to talk about how the GPU...\n",
            "[364.36 - 385.43]: does this and how we as programmers harness the GPU to achieve what we want to do. So we can get fantastic images like Mario there. I'm a bit of a Nintendo fan so there will be a few pictures of Mario and Zelda throughout. I'm guessing that are mostly PC players. No, a few shaking heads. Who's a PC game player?\n",
            "[385.61 - 392.14]: Yeah, and with Hans Guur. Why do I hate PC game development as a game developer?\n",
            "[392.19 - 393.02]: Thank you.\n",
            "[393.06 - 393.42]: Thank you.\n",
            "[393.69 - 418.94]: Because they're all different.\n",
            "[418.99 - 427.13]: As mentioned this lecture is just kind of an introduction to this whole process. So we'll give you an overview\n",
            "[427.19 - 428.64]: of each step.\n",
            "[428.70 - 433.77]: along the way, give a sense of what the module is and what's expected from the coursework as well.\n",
            "[433.79 - 442.04]: So, as was mentioned by my friend at the back there, our models in the game, the 3D models...\n",
            "[442.07 - 455.86]: They're built by from a sequence of polygons, typically triangles. So there's Mario again. We all know he's built up a mesh of triangles, the quartz here.\n",
            "[455.98 - 457.82]: Of course, just two triangles.\n",
            "[457.85 - 459.09]: Put together.\n",
            "[459.82 - 461.98]: It's actually it's more\n",
            "[462.18 - 468.08]: abstract than that. The models aren't really sets of triangles, they're sets of points in space.\n",
            "[468.18 - 477.32]: And those points in space are connected together in polygon. It's usually in triangles. You can get a string of triangles to put together to make the model so item.\n",
            "[477.44 - 480.62]: A point in space is known as a vertex.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "zxs6wz_0HJcE",
        "outputId": "89aeff0c-c659-4c77-a93d-a3f5ce4e64ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I don't understand what you're asking. Can you please clarify your question?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-060b35c0b6a7>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prompt: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
        "PERSIST = False\n",
        "\n",
        "query = None\n",
        "if len(sys.argv) > 1:\n",
        "  query = sys.argv[1]\n",
        "\n",
        "if PERSIST and os.path.exists(\"persist\"):\n",
        "  print(\"Reusing index...\\n\")\n",
        "  vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
        "  index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
        "else:\n",
        "  loader = TextLoader(\"Recording3.txt\") # Use this line if you only need data.txt\n",
        "  #loader = DirectoryLoader(\"data/\")\n",
        "  if PERSIST:\n",
        "    index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\n",
        "  else:\n",
        "    index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "  llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
        "  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "  if not query:\n",
        "    query = input(\"Prompt: \")\n",
        "  if query in ['quit', 'q', 'exit']:\n",
        "    sys.exit()\n",
        "  result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "  print(result['answer'])\n",
        "\n",
        "  chat_history.append((query, result['answer']))\n",
        "  query = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# دالة للرد باستخدام GPT\n",
        "def get_gpt_response(user_input):\n",
        "    conversation_history = f\"User: {user_input}\\nBot:\"\n",
        "    gpt_response = ChatOpenAI(model=\"gpt-3.5-turbo\").predict(conversation_history)  # استخدام GPT للرد على السؤال\n",
        "    print(f\"Additional information: {gpt_response}\")\n",
        "    return gpt_response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# قم بدمج هذه الدالة في الشيفرة السابقة\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
        "PERSIST = False\n",
        "\n",
        "query = None\n",
        "if len(sys.argv) > 1:\n",
        "    query = sys.argv[1]\n",
        "\n",
        "if PERSIST and os.path.exists(\"persist\"):\n",
        "    print(\"Reusing index...\\n\")\n",
        "    vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
        "    index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
        "else:\n",
        "    loader = TextLoader(\"Recording3.txt\")  # Use this line if you only need data.txt\n",
        "    # loader = DirectoryLoader(\"data/\")\n",
        "    if PERSIST:\n",
        "        index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\": \"persist\"}).from_loaders([loader])\n",
        "    else:\n",
        "        index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
        "    retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "    if not query:\n",
        "        query = input(\"Prompt: \")\n",
        "    if query in ['quit', 'q', 'exit']:\n",
        "        sys.exit()\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "    print(result['answer'])\n",
        "\n",
        "    gpt_response = get_gpt_response(query)\n",
        "    chat_history.append((query, result['answer'], gpt_response))\n",
        "    query = None\n"
      ],
      "metadata": {
        "id": "igoZAIUuZlIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
        "PERSIST = False\n",
        "\n",
        "query = None\n",
        "if len(sys.argv) > 1:\n",
        "    query = sys.argv[1]\n",
        "\n",
        "if PERSIST and os.path.exists(\"persist\"):\n",
        "    print(\"Reusing index...\\n\")\n",
        "    vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
        "    index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
        "else:\n",
        "    loader = TextLoader(\"Recording3.txt\")  # Use this line if you only need data.txt\n",
        "    # loader = DirectoryLoader(\"data/\")\n",
        "    if PERSIST:\n",
        "        index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\": \"persist\"}).from_loaders([loader])\n",
        "    else:\n",
        "        index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
        "    retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "    if not query:\n",
        "        query = input(\"Prompt: \")\n",
        "\n",
        "    if query in ['quit', 'q', 'exit']:\n",
        "        sys.exit()\n",
        "\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    if not result['answer']:\n",
        "        # If no answer from the file, generate a response using GPT\n",
        "        user_input = f\"User: {query}\\nBot:\"\n",
        "        gpt_response = chain.llm.predict(user_input)\n",
        "        print(f\"Additional information: {gpt_response}\")\n",
        "        result['answer'] = gpt_response\n",
        "\n",
        "        if \"I don't have enough information\" in result['answer']:\n",
        "            # If the response is still \"I don't have enough information,\" try GPT external\n",
        "            gpt_response_external = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": query},\n",
        "                ],\n",
        "            )\n",
        "            result['answer'] = gpt_response_external['choices'][0]['message']['content']\n",
        "            print(\"Additional information (external):\", result['answer'])\n",
        "\n",
        "    print(result['answer'])\n",
        "\n",
        "    chat_history.append((query, result['answer']))\n",
        "    query = None\n"
      ],
      "metadata": {
        "id": "waA4DxAR9Oiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
        "PERSIST = False\n",
        "\n",
        "query = None\n",
        "if len(sys.argv) > 1:\n",
        "    query = sys.argv[1]\n",
        "\n",
        "if PERSIST and os.path.exists(\"persist\"):\n",
        "    print(\"Reusing index...\\n\")\n",
        "    vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
        "    index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
        "else:\n",
        "    loader = TextLoader(\"Recording3.txt\")  # Use this line if you only need data.txt\n",
        "    # loader = DirectoryLoader(\"data/\")\n",
        "    if PERSIST:\n",
        "        index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\": \"persist\"}).from_loaders([loader])\n",
        "    else:\n",
        "        index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
        "    retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "    if not query:\n",
        "        query = input(\"Prompt: \")\n",
        "\n",
        "    if query in ['quit', 'q', 'exit']:\n",
        "        sys.exit()\n",
        "\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    if result['answer'] and \"I don't know\" not in result['answer'] and \"I don't have enough\" not in result['answer'] and \"I'm sorry\" not in result['answer']and \"There is no mention\" not in result['answer']:\n",
        "        # If there is a valid answer from the file, use it\n",
        "        print(result['answer'])\n",
        "    else:\n",
        "        # If the response is still \"I don't have enough information,\" try GPT external\n",
        "        client = OpenAI()\n",
        "        gpt_response_external = response = client.completions.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=query\n",
        "            )\n",
        "\n",
        "\n",
        "        result['answer'] = gpt_response_external.choices[0].text\n",
        "        print(\"Additional information (external from GPT-3.5 Turbo):\", result['answer'])\n",
        "\n",
        "    chat_history.append((query, result['answer']))\n",
        "    query = None\n"
      ],
      "metadata": {
        "id": "_Z25RFhoLQtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "file_path = \"/content/Recording3.txt\"  # تعيين مسار الملف النصي الذي تريد استخراج النص منه\n",
        "\n",
        "# تحميل التوكنايزر والنموذج\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# استخراج النص من الملف\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()  # قراءة الملف النصي بأكمله وتخزين النص في متغير text\n",
        "\n",
        "    # تحويل النص إلى الترميز المناسب لـ BART وطباعة النص\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(\"الملخص:\")\n",
        "    print(output)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"لم يتم العثور على الملف في المسار: {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"حدث خطأ أثناء معالجة الملف: {e}\")\n"
      ],
      "metadata": {
        "id": "vyhwHnmykXmf",
        "outputId": "df5a958a-3362-49cd-9546-89aacbb6259c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "449d0df296fc4db29c4b74f8b285d6ce",
            "2ff32ef7db77416ebe1fab85bc1a9b77",
            "d43aa137351c41e0ac0ff4629feb7367",
            "2ef73da4792d41d4ad7da86fca9f0934",
            "b7ee0600291541ba862838e602226ffa",
            "f4ecf0b6164c43609e09fe12b2ff0407",
            "41b9e23b23f1440ca6c0cfa35a230032",
            "092c340632af4a858f0448fcea295416",
            "f6b34db2eb3a4b38a3965daeea8ea12a",
            "22913a702c254aba992a9722d0435af4",
            "6a3068153ca442dda298d48a18aaa866"
          ]
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "449d0df296fc4db29c4b74f8b285d6ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الملخص:\n",
            "The course is based on the popular game World of War 2. The lectures are about how to turn 3D models created by artists into a 2D image. The course will also cover how to use the Unity game engine. It is expected to be completed by the end of September.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "449d0df296fc4db29c4b74f8b285d6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ff32ef7db77416ebe1fab85bc1a9b77",
              "IPY_MODEL_d43aa137351c41e0ac0ff4629feb7367",
              "IPY_MODEL_2ef73da4792d41d4ad7da86fca9f0934"
            ],
            "layout": "IPY_MODEL_b7ee0600291541ba862838e602226ffa"
          }
        },
        "2ff32ef7db77416ebe1fab85bc1a9b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ecf0b6164c43609e09fe12b2ff0407",
            "placeholder": "​",
            "style": "IPY_MODEL_41b9e23b23f1440ca6c0cfa35a230032",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "d43aa137351c41e0ac0ff4629feb7367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_092c340632af4a858f0448fcea295416",
            "max": 1625270765,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6b34db2eb3a4b38a3965daeea8ea12a",
            "value": 1625270765
          }
        },
        "2ef73da4792d41d4ad7da86fca9f0934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22913a702c254aba992a9722d0435af4",
            "placeholder": "​",
            "style": "IPY_MODEL_6a3068153ca442dda298d48a18aaa866",
            "value": " 1.63G/1.63G [00:13&lt;00:00, 125MB/s]"
          }
        },
        "b7ee0600291541ba862838e602226ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ecf0b6164c43609e09fe12b2ff0407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b9e23b23f1440ca6c0cfa35a230032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "092c340632af4a858f0448fcea295416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b34db2eb3a4b38a3965daeea8ea12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22913a702c254aba992a9722d0435af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3068153ca442dda298d48a18aaa866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
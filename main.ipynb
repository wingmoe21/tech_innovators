{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingmoe21/tech_innovators/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TAO1wvYSKBk",
        "outputId": "a57669f6-4f98-4778-829a-0fad3ade14e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/4.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/4.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m3.8/4.2 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140081 sha256=b406045a5b25597823b5b0933d6135e19328ee768892e2463d933c7e9456e590\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bOylMqkSjfF",
        "outputId": "72e3ef1c-e0ad-4545-a648-c2ac1b17be97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NykBOPcYKPg",
        "outputId": "b4ac023d-a48d-49b8-91e1-6123b213576c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPeBNAyZYOst",
        "outputId": "428f38e3-d75c-4b4d-8be4-5f080195584c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2\n",
            "Err:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2\n",
            "  404  Not Found [IP: 91.189.91.82 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/poppler/poppler-utils_22.02.0-2ubuntu0.2_amd64.deb  404  Not Found [IP: 91.189.91.82 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ],
      "source": [
        "!apt-get install poppler-utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "xbqKvIy4YswB",
        "outputId": "2e211804-02ca-4959-81c8-8f779bb35b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-vision\n",
            "  Downloading google_cloud_vision-3.4.5-py2.py3-none-any.whl (444 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/444.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/444.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.1/444.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.61.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.59.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (0.5.0)\n",
            "Installing collected packages: google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.4.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install google-cloud-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NdLPeHXxMMmY",
        "outputId": "eaf7341d-2e33-4da7-df4e-daea653fa921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/m-bain/whisperx.git\n",
            "  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-9hnsifcq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-9hnsifcq\n",
            "  Resolved https://github.com/m-bain/whisperx.git to commit d97cdb7bcf302fb3e1651321a5935f90594e994c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.1.0+cu118)\n",
            "Collecting faster-whisper>=0.8 (from whisperx==3.1.1)\n",
            "  Downloading faster_whisper-0.9.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (4.35.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (1.5.3)\n",
            "Requirement already satisfied: setuptools>=65 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (67.7.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (3.8.1)\n",
            "Collecting pyannote.audio==3.1.0 (from whisperx==3.1.1)\n",
            "  Downloading pyannote.audio-3.1.0-py2.py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.6/208.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asteroid-filterbanks>=0.4 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Collecting einops>=0.6.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.0->whisperx==3.1.1) (0.19.4)\n",
            "Collecting lightning>=2.0.1 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<3.0,>=2.1 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.core>=5.0.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.database>=5.0.1 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading pyannote.database-5.0.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.metrics>=3.2 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.pipeline>=3.0.1 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.0->whisperx==3.1.1) (13.7.0)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.0->whisperx==3.1.1) (0.12.1)\n",
            "Collecting speechbrain>=0.5.14 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading speechbrain-0.5.16-py3-none-any.whl (630 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.6/630.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=2.6 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-audiomentations>=0.11.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.11.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av==10.* (from faster-whisper>=0.8->whisperx==3.1.1)\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctranslate2<4,>=3.17 (from faster-whisper>=0.8->whisperx==3.1.1)\n",
            "  Downloading ctranslate2-3.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.15,>=0.13 (from faster-whisper>=0.8->whisperx==3.1.1)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<2,>=1.14 (from faster-whisper>=0.8->whisperx==3.1.1)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (2.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (0.4.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning>=2.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper>=0.8->whisperx==3.1.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper>=0.8->whisperx==3.1.1) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper>=0.8->whisperx==3.1.1) (3.20.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.0->whisperx==3.1.1) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.0->whisperx==3.1.1) (1.11.3)\n",
            "Requirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio==3.1.0->whisperx==3.1.1) (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (1.2.2)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (3.7.1)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->whisperx==3.1.1) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.0->whisperx==3.1.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.0->whisperx==3.1.1) (2.16.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio==3.1.0->whisperx==3.1.1) (1.16.0)\n",
            "Collecting hyperpyyaml (from speechbrain>=0.5.14->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Collecting sentencepiece (from speechbrain>=0.5.14->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->whisperx==3.1.1) (1.3.0)\n",
            "Collecting huggingface-hub>=0.13.0 (from pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (0.10.1)\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->whisperx==3.1.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (2023.7.22)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.1.0->whisperx==3.1.1) (2.21)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->torch>=2->whisperx==3.1.1) (3.8.6)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (1.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.1.0->whisperx==3.1.1) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (3.1.1)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.0->whisperx==3.1.1) (2.0.23)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio==3.1.0->whisperx==3.1.1) (3.2.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper>=0.8->whisperx==3.1.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=2->whisperx==3.1.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=2->whisperx==3.1.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=2->whisperx==3.1.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=2->whisperx==3.1.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=2->whisperx==3.1.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch>=2->whisperx==3.1.1) (1.3.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.0->whisperx==3.1.1) (4.0.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.0->whisperx==3.1.1)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.0->whisperx==3.1.1) (3.0.1)\n",
            "Building wheels for collected packages: whisperx, antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisperx: filename=whisperx-3.1.1-py3-none-any.whl size=36304 sha256=6cf521a42934ef8de53700cb1b69d683c2eb592b6221c553196817e7754e5cf8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fvxbbjr2/wheels/27/fb/53/682b85073a466f1866910d7257233e53b0cc126ab50e7c5373\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=b1ba54f56933322dccd54215c552c420ab8961f210c54915665beba675b2e064\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=55166ca3f95a829b7d8f8aa9ada2f229e41acc2ecb1b5a90250d0377a102c9b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=1f9df78761691e8b8f1dad0c639f5aad933bd62a127ee05d70aeac88c3cd4d29\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
            "Successfully built whisperx antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: sentencepiece, primePy, docopt, av, antlr4-python3-runtime, tensorboardX, shellingham, semver, ruamel.yaml.clib, omegaconf, Mako, lightning-utilities, humanfriendly, einops, ctranslate2, colorlog, colorama, ruamel.yaml, pyannote.core, huggingface-hub, coloredlogs, alembic, torchmetrics, tokenizers, pytorch-metric-learning, optuna, onnxruntime, julius, hyperpyyaml, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.database, faster-whisper, torch-audiomentations, pyannote.pipeline, pyannote.metrics, lightning, pyannote.audio, whisperx\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "Successfully installed Mako-1.3.0 alembic-1.12.1 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 av-10.0.0 colorama-0.4.6 coloredlogs-15.0.1 colorlog-6.7.0 ctranslate2-3.22.0 docopt-0.6.2 einops-0.7.0 faster-whisper-0.9.0 huggingface-hub-0.17.3 humanfriendly-10.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.1.2 lightning-utilities-0.10.0 omegaconf-2.3.0 onnxruntime-1.16.3 optuna-3.4.0 primePy-1.3 pyannote.audio-3.1.0 pyannote.core-5.0.0 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.1.2 pytorch-metric-learning-2.3.0 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 semver-3.0.2 sentencepiece-0.1.99 shellingham-1.5.4 speechbrain-0.5.16 tensorboardX-2.6.2.2 tokenizers-0.14.1 torch-audiomentations-0.11.0 torch-pitch-shift-1.2.4 torchmetrics-1.2.0 whisperx-3.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install git+https://github.com/m-bain/whisperx.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNgFtZB-M5K0",
        "outputId": "4b0af34b-6ed5-4f63-8f87-f85f381f0a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvHPmkDeNGiJ",
        "outputId": "88471e0a-c1b2-4e47-9b79-7b61b37e690b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=fcdc7d619d7895040e294584d30a1aa80095f35c08ceba5cce38d781702487b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iWOk5akTbTBf",
        "outputId": "ee4c2497-7f7e-442e-a54c-e53b5355c1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting frontend\n",
            "  Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting starlette>=0.12.0 (from frontend)\n",
            "  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn>=0.7.1 (from frontend)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.1.2)\n",
            "Collecting aiofiles (from frontend)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-6.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (4.0.2)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (1.5.3)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.7.1->frontend)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->fitz) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.1.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (67.7.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.2.1)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.0.0-py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson>=3.8.0 (from nipype->fitz)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.13.1)\n",
            "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Collecting looseversion (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2023.3.post1)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (0.18.3)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.1.3)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2023.7.22)\n",
            "Installing collected packages: looseversion, traits, simplejson, isodate, h11, configparser, configobj, ci-info, aiofiles, uvicorn, starlette, rdflib, pyxnat, etelemetry, prov, frontend, nipype, fitz\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 ci-info-0.3.0 configobj-5.0.8 configparser-6.0.0 etelemetry-0.3.1 fitz-0.0.1.dev2 frontend-0.0.3 h11-0.14.0 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 prov-2.0.0 pyxnat-1.6 rdflib-7.0.0 simplejson-3.19.2 starlette-0.32.0.post1 traits-6.3.2 uvicorn-0.24.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install frontend fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGulnrbud4J-",
        "outputId": "6802ebbe-aa3b-4328-eae7-95f0e5c38c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.6-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.6 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.23.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.6 pymupdf-1.23.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aGy74H69yDfY",
        "outputId": "1a451308-a8e9-46a7-e610-844b19f956b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "e73e67fe82ec472f976fa945b9777111",
            "ff821f4e722c438a89d118786bc2e55b",
            "f8d438b81f11493ebde058befbfd0e99",
            "e05312b9b710438eb3d2f1c2a9a48053",
            "b682cdb8611f472295dc4e0362ff56ca",
            "ebbaac86ca94436a9f8079ea05acd975",
            "99b30b4fd83648319f95bf65ae8354ea",
            "aedeb29e9dc248d58fbcf723d520179e",
            "b5f7409b3cf64020a521be2a86ca09c3",
            "ff9f8b98b92240d2a6edb6688ecfc2c7",
            "1b0e15a208bf4285a2bdc5a3b50e7b2b",
            "72ea9e32138e49619e70fe1265b8ceff",
            "4271b113ac014323b704588abe7879d0",
            "6e21d935032f4c158bba0a0e3af5f970",
            "346b17b11c754422b06e57e9e6865ef5",
            "9e579e8584ff43a4b4143902e69f7f29",
            "6be544bcd6674770997cae6949b38039",
            "5a9ebd2c967b49d7bf2388d1019a7ddd",
            "95266ef860f041e7bf8947635ae87082",
            "8c4b4e545f6941fc8582003fa8c2199c",
            "f4771587fd314a5b8436c4c897fba6ac",
            "29a0ed056a8e41fea4f77928befd2483",
            "5afd81fba4864e81b44ae277bee2b652",
            "ae704d3efe3b42c8a7487561849e2629",
            "c85611473faf401e87c26b854f5026e5",
            "1f96250ba3b44200b5bb27421b180283",
            "455124d5d85d433e8ade08327d4a8e71",
            "4d5a13a361d3488982170cfdec34874c",
            "24d724db6ba440b9a946a78305b591a1",
            "2122f1f49e00491abc95dcc601ad57c3",
            "e70cc0b29e384b809f37bd768efa185d",
            "ca2085ef83e44f758540ffa4bb0c894a",
            "9895504404ec4ad4875a0f002f0cb993",
            "173f3ff51bbd4c1ba7c0a1fd60f0b04f",
            "ce99cdfc35b24c91adf5cf0fd66232ca",
            "ec16eee49416491c979beaf7da074cec",
            "8631b889a59c4c91afef3f3ebd5cacd3",
            "4e09bb956ac4434b9ebe9730bc6e8a13",
            "5089df5f30344f4db6b8686292869fe3",
            "e8da9a284f6147f4ae619a478b17a6be",
            "141d9328877142f6adc5e5a0ffc74164",
            "11ff01d46d77470a83cfafa8c93db44d",
            "7521b3bd0412471c80a93c9c9397dd1d",
            "4f675feff8aa41e3bb266e22bb3f560b"
          ]
        },
        "id": "HnoexEOtL5Sq",
        "outputId": "7e025edf-f03f-4e1f-bedb-203ad05da8c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e73e67fe82ec472f976fa945b9777111",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/2.80k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72ea9e32138e49619e70fe1265b8ceff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5afd81fba4864e81b44ae277bee2b652",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "173f3ff51bbd4c1ba7c0a1fd60f0b04f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 16.9M/16.9M [00:00<00:00, 110MiB/s]\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.99) in first 30s of audio...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
            "100%|██████████| 360M/360M [00:01<00:00, 266MB/s]\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "import cv2\n",
        "import fitz\n",
        "import whisperx\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "device = \"cuda\"\n",
        "video_path = \"/content/What is evaluation order of function parameters in C.mp4\"\n",
        "batch_size = 10  # reduce if low on GPU mem\n",
        "compute_type = \"float16\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "# Tolerance to decide whether two images are similar\n",
        "similarity_threshold = 0.95\n",
        "\n",
        "\n",
        "def convert_mp4_to_mp3(input_file, output_file):\n",
        "    video_clip = VideoFileClip(input_file)\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile(output_file)\n",
        "    audio_clip.close()\n",
        "\n",
        "\n",
        "def whisberX(vid):\n",
        "    convert_mp4_to_mp3(vid, 'output.mp3')\n",
        "\n",
        "    # 1. Transcribe with original whisper (batched)\n",
        "    model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "\n",
        "    audio = whisperx.load_audio('output.mp3')\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "\n",
        "    # 2. Align whisper output\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "    # Save the aligned segments to a text file\n",
        "    output_file_path = 'aligned_segments.txt'\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        for segment in result[\"segments\"]:\n",
        "            f.write(f\"{segment['start']} - {segment['end']}: {segment['text']}\\n\")\n",
        "\n",
        "\n",
        "def are_frames_similar(frame_pair):\n",
        "    frame1, frame2 = frame_pair\n",
        "    # Calculate the structural similarity index (SSI) between two frames\n",
        "    # You can use skimage's structural_similarity function for this\n",
        "    # from skimage.metrics import structural_similarity as compare_ssim\n",
        "    # Return True if they are similar above the given threshold\n",
        "    ssi_index, _ = ssim(frame1, frame2, full=True)\n",
        "    return ssi_index > similarity_threshold\n",
        "\n",
        "\n",
        "def OCR(vid):\n",
        "\n",
        "    # Previous frame for comparison\n",
        "    prev_frame = None\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        frames.append(gray_frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    similar_frames = []\n",
        "    frame_pairs = zip(frames[:-1], frames[1:])\n",
        "    for frame_pair in frame_pairs:\n",
        "        similar_frames.append(are_frames_similar(frame_pair))\n",
        "\n",
        "    unique_slides = []\n",
        "\n",
        "    for i, is_similar in enumerate(similar_frames):\n",
        "        if not is_similar:\n",
        "            slide_image = Image.fromarray(frames[i + 1])\n",
        "            unique_slides.append(slide_image)\n",
        "\n",
        "    return unique_slides\n",
        "def save_slides_to_pdf(unique_slides):\n",
        "    pdf_path = 'sds.pdf'\n",
        "    pdf_doc = fitz.open()\n",
        "\n",
        "    for slide in unique_slides:\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        slide.save(img_byte_arr, format='PNG')\n",
        "        img_byte_arr = img_byte_arr.getvalue()\n",
        "        img = fitz.open(\"png\", img_byte_arr)\n",
        "        rect = img[0].rect\n",
        "        pdfbytes = img.convert_to_pdf()\n",
        "        img.close()\n",
        "        imgPDF = fitz.open(\"pdf\", pdfbytes)\n",
        "        page = pdf_doc.new_page(width=rect.width, height=rect.height)\n",
        "        page.show_pdf_page(rect, imgPDF, 0)\n",
        "\n",
        "    pdf_doc.save(pdf_path)\n",
        "    pdf_doc.close()\n",
        "\n",
        "whisberX(video_path)\n",
        "unique_slides = OCR(video_path)\n",
        "save_slides_to_pdf(unique_slides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YqckEJEVZPhB"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAJJAdl2ZYf4",
        "outputId": "f8768180-47a8-4bd8-f359-d4857f7c489a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project capstone-405521\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Fz_ljAoNZys7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/capstone-405521-be463f8a80c5.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KoewL0HPhc6",
        "outputId": "fc55a8df-2327-4366-dcee-11ad0a41b341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['image_page_0_img_0.png', 'image_page_1_img_0.png', 'image_page_2_img_0.png', 'image_page_3_img_0.png', 'image_page_4_img_0.png', 'image_page_5_img_0.png', 'image_page_6_img_0.png', 'image_page_7_img_0.png', 'image_page_8_img_0.png', 'image_page_9_img_0.png', 'image_page_10_img_0.png', 'image_page_11_img_0.png', 'image_page_12_img_0.png', 'image_page_13_img_0.png', 'image_page_14_img_0.png', 'image_page_15_img_0.png', 'image_page_16_img_0.png', 'image_page_17_img_0.png']\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_images_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    images = []\n",
        "    for i in range(len(doc)):\n",
        "        for img_index, img in enumerate(doc.get_page_images(i)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Constructing a filename for each image\n",
        "            image_filename = f\"image_page_{i}_img_{img_index}.png\"\n",
        "\n",
        "            with open(image_filename, \"wb\") as image_file:\n",
        "                image_file.write(image_bytes)\n",
        "\n",
        "            images.append(image_filename)\n",
        "    return images\n",
        "\n",
        "# Replace 'your_pdf_file.pdf' with the path to your PDF file.\n",
        "images1 = extract_images_from_pdf(\"/content/extracted_slides (2).pdf\")\n",
        "print(images1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIPgPAkHXp5m",
        "outputId": "781b88c2-3655-4e8b-8b64-77444ffa5fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next Greater Element\n",
            "Given an array, print the Next Greater Element (NGE) for every element.\n",
            "The Next greater Element for an element x is the first greater element on the\n",
            "right side of x in array.\n",
            "Elements for which no greater element exist, consider next greater element\n",
            "as -1.\n",
            "Next Greater Element\n",
            "For any array, rightmost element always has next greater element as -1.\n",
            "For an array which is sorted in decreasing order, all elements have next greater\n",
            "element as -1.\n",
            "For the input array [4, 5, 2, 25}, the next greater elements for each element are\n",
            "as follows.\n",
            "Element\n",
            "4\n",
            "5\n",
            "2\n",
            "25\n",
            "-->\n",
            "-->\n",
            "-->\n",
            "-->\n",
            "NGE\n",
            "5\n",
            "25\n",
            "25\n",
            "-1\n",
            "Next Greater Element\n",
            "For the input array [13, 7, 6, 12}, the next greater elements for each element\n",
            "are as follows.\n",
            "Element\n",
            "13\n",
            "7\n",
            "6\n",
            "12\n",
            "-->\n",
            "-->\n",
            "-->\n",
            "-->\n",
            "NGE\n",
            "-1\n",
            "12\n",
            "12\n",
            "-1\n",
            "Method 1 (Simple)\n",
            "Use two loops:\n",
            "The outer loop picks all the elements one by one.\n",
            "The inner loop looks for the first greater element for the element picked\n",
            "by outer loop.\n",
            "➤ If a greater element is found then that element is printed as next,\n",
            "otherwise -1 is printed.\n",
            "/* prints element and NGE pair for all elements of\n",
            "arr[] of size n */\n",
            "void printNGE (int arr[], int n)\n",
            "{\n",
            "}\n",
            "int next, i, j;\n",
            "for (i=0; i<n; i++)\n",
            "{\n",
            "}\n",
            "next = -1;\n",
            "for (j\n",
            "=\n",
            "{\n",
            "}\n",
            "Method 1 (Simple)\n",
            "i+1; j<n; j++)\n",
            "if (arr[i] < arr[j])\n",
            "{\n",
            "next = arr[j];\n",
            "break;\n",
            "}\n",
            "printf(\"%d %d\\n\", arr[i], next);\n",
            "--\n",
            "int main()\n",
            "{\n",
            "}\n",
            "int arr[] = {11, 13, 21, 3};\n",
            "int n = sizeof(arr)/sizeof(arr [0]);\n",
            "printNGE (arr, n);\n",
            "getchar();\n",
            "return 0;\n",
            "Method 2 (Using Stack)\n",
            "1) Push the first element to stack.\n",
            "2)\n",
            "Pick rest of the elements one by one and follow following steps in loop.\n",
            "1) Mark the current element as next.\n",
            "2) If stack is not empty, then pop an element from stack and compare it with next.\n",
            "3) If next is greater than the popped element, then next is the next greater element\n",
            "for the popped element.\n",
            "4)\n",
            "Keep popping from the stack while the popped element is smaller than next. next\n",
            "becomes the next greater element for all such popped elements\n",
            "5) If next is smaller than the popped element, then push the popped element back.\n",
            "3) After the loop in step 2 is over, pop all the elements from stack and print -1 as next\n",
            "element for them.\n",
            "Method 2 (Using Stack)\n",
            "4\n",
            "5 2 25\n",
            "Method 2 (Using Stack)\n",
            "// A Stack based C program to find next greater element\n",
            "// for all array elements.\n",
            "#include<stdio.h>\n",
            "#include<stdlib.h>\n",
            "#define STACKSIZE 100\n",
            "// stack structure\n",
            "struct stack\n",
            "{\n",
            "};\n",
            "int top;\n",
            "int items [STACKSIZE];\n",
            "Method 2 (Using Stack)\n",
            "// Stack Functions to be used by printNGE ()\n",
            "void push (struct stack *ps, int x)\n",
            "{\n",
            "}\n",
            "if (ps->top == STACKSIZE-1)\n",
            "{\n",
            "}\n",
            "else\n",
            "{\n",
            "}\n",
            "printf(\"Error: stack overflow\\n\");\n",
            "getchar();\n",
            "exit(0);\n",
            "ps->top += 1;\n",
            "int top = ps->top;\n",
            "ps->items [top] = x;\n",
            "int pop(struct stack *ps).\n",
            "{\n",
            "}\n",
            "int temp;\n",
            "if (ps->top == -1)\n",
            "{\n",
            "}\n",
            "else\n",
            "{\n",
            "Method 2 (Using Stack)\n",
            "}\n",
            "printf(\"Error: stack underflow \\n\");\n",
            "getchar();\n",
            "exit (0);\n",
            "int top = ps->top;\n",
            "temp = ps->items [top];\n",
            "ps->top = 1;\n",
            "return temp;\n",
            "bool isEmpty (struct stack *ps)\n",
            "{\n",
            "}\n",
            "return (ps->top == -1)? true : false;\n",
            "Method 2 (Using Stack)\n",
            "/* prints element and NGE pair for all elements of\n",
            "arr[] of size n */\n",
            "void printNGE (int arr[], int n)\n",
            "{\n",
            "int i = 0;\n",
            "struct stack s;\n",
            "s.top = -1;\n",
            "int element, next;\n",
            "/* push the first element to stack */\n",
            "push(&s, arr[0]);\n",
            "Method 2 (Using Stack)\n",
            "// iterate for rest of the elements.\n",
            "for (i=1; i<n; i++)\n",
            "{\n",
            "next = arr[i];\n",
            "if (isEmpty(&s)\n",
            "{\n",
            "false)\n",
            "// if stack is not empty, then pop an element from stack\n",
            "element = pop(&s);\n",
            "==\n",
            "/* If the popped element is smaller than next, then\n",
            "a) print the pair\n",
            "b) keep popping while elements are smaller and\n",
            "stack is not empty */\n",
            "}\n",
            "Method 2 (Using Stack)\n",
            "while (element < next)\n",
            "{\n",
            "}\n",
            "printf(\"\\n %d --> %d\", element, next);\n",
            "if(isEmpty(&s)\n",
            "true)\n",
            "break;\n",
            "element = pop(&s);\n",
            "/*\n",
            "If element is greater than next, then push\n",
            "the element back */\n",
            "(element > next)\n",
            "push(&s, element);\n",
            "if\n",
            "}\n",
            "/* push next to stack so that we can find\n",
            "next greater for it */\n",
            "push(&s, next);\n",
            "Method 2 (Using Stack)\n",
            "/* After iterating over the loop, the remaining\n",
            "elements in stack do not have the next greater\n",
            "element, so print -1 for them */\n",
            "while (isEmpty (&s) == false)\n",
            "{\n",
            "}\n",
            "element = pop (&s);\n",
            "next = -1;\n",
            "printf(\"\\n%d %d\", element, next);\n",
            "--\n",
            "Method 2 (Using Stack)\n",
            "/* Driver program to test above functions */\n",
            "int main()\n",
            "{\n",
            "}\n",
            "int arr [] = {11, 13, 21, 3};\n",
            "int n = sizeof(arr)/sizeof(arr[0]);\n",
            "printNGE (arr, n);\n",
            "getchar();\n",
            "return 0;\n",
            "Method 2 (Using Stack)\n",
            "Time Complexity: O(n). The worst case occurs when all elements are sorted in\n",
            "decreasing order. If elements are sorted in decreasing order, then every\n",
            "element is processed at most 4 times.\n",
            "Initially pushed to the stack.\n",
            "Popped from the stack when next element is being processed.\n",
            "Pushed back to the stack because next element is smaller.\n",
            "➤ Popped from the stack in step 3 of algo.\n",
            "Summary\n",
            "Method\n",
            "METHOD 1 (Linear Search)\n",
            "Method 2 (Using Stack)\n",
            "Time Complexity\n",
            "O(n*n)\n",
            "O(n)\n",
            "Thank you for watching!\n",
            "Please leave us your comments.\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "import cv2\n",
        "import fitz\n",
        "import PyPDF2\n",
        "from google.cloud import vision\n",
        "from moviepy.editor import VideoFileClip\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "\n",
        "# Function to convert PDF pages to images\n",
        "def convert_pdf_to_images(pdf_path):\n",
        "    return convert_from_path(pdf_path)\n",
        "\n",
        "pdf_path = '/content/extracted_slides (2).pdf'\n",
        "images = convert_pdf_to_images(pdf_path)\n",
        "\n",
        "\n",
        "# Initialize the Google Vision API client\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# Function to interpret an image\n",
        "def interpret_image(image_content):\n",
        "    content = io.BytesIO()\n",
        "    image_content.save(content, format='JPEG')\n",
        "    content = content.getvalue()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "    response = client.text_detection(image=image)\n",
        "\n",
        "    return response.text_annotations[0].description\n",
        "\n",
        "# Interpreting each image\n",
        "for image in images:\n",
        "    description = interpret_image(image)\n",
        "    print(description)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "# OpenAI API Key\n",
        "api_key = \"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\"\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/content/image_page_0_img_0 (4).png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "  \"model\": \"gpt-4-vision-preview\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"What’s in this image?\"\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ],\n",
        "  \"max_tokens\": 300\n",
        "}\n",
        "\n",
        "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "-5D-H6hwZyzD",
        "outputId": "881cf188-a515-49ec-e11c-87658b7e8269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-8MjHd9TiNkn56E3r2kUP4SuCuBfFh', 'object': 'chat.completion', 'created': 1700427977, 'model': 'gpt-4-1106-vision-preview', 'usage': {'prompt_tokens': 1118, 'completion_tokens': 241, 'total_tokens': 1359}, 'choices': [{'message': {'role': 'assistant', 'content': \"The image contains text that seems to refer to a programming concept, specifically in the C language. There's a piece of code that demonstrates a function call with arguments that may have side effects due to their sequence of evaluation. The code sample is as follows:\\n\\n```\\nvoid func (int, int);\\n\\nint i = 2;\\nfunc (i++, i++);\\n```\\n\\nBelow the code, there's a discussion about the different evaluation order outputs of this program:\\n\\n- From left to right\\n- From right to left\\n\\nThen, there's text describing that the order of evaluation of arguments in a function call is compiler dependent in C. This means that it isn't specified by the C standard, and different compilers may evaluate arguments in different orders, which can result in different behaviors when side effects are involved, as with the `i++` increment operation in the given example.\\n\\nThe last part of the text emphasizes that it is never safe to depend on the order of evaluation of such side effects, because a function call like the one shown above may behave differently from one compiler to another. This is an important concept in C programming as it pertains to undefined behavior and portability of code across different platforms.\"}, 'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import fitz\n",
        "import whisperx\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "device = \"cuda\"\n",
        "batch_size = 10  # reduce if low on GPU mem\n",
        "compute_type = \"float16\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "# Tolerance to decide whether two images are similar\n",
        "similarity_threshold = 0.95\n",
        "paths = []\n",
        "for file in os.listdir('/content/drive/MyDrive/VID'):\n",
        "  paths.append(os.path.join('/content/drive/MyDrive/VID', file))\n",
        "\n",
        "for i in range(len(paths)):\n",
        "    video_clip = VideoFileClip(paths[i])\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile('output.mp3')\n",
        "    audio_clip.close()\n",
        "\n",
        "    # 1. Transcribe with original whisper (batched)\n",
        "    model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "\n",
        "    audio = whisperx.load_audio('output.mp3')\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "\n",
        "    # 2. Align whisper output\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "    # Save the aligned segments to a text file\n",
        "    file_name = paths[i].split('/')[-1].split('.')[0]\n",
        "    output_file_path = f'{file_name}.txt'\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        for segment in result[\"segments\"]:\n",
        "            f.write(f\"{segment['start']} - {segment['end']}: {segment['text']}\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAW95AzbyLyD",
        "outputId": "e1391852-a941-4b1a-9990-de6895573322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "48f0be16881e410e8093af6479f41757",
            "65e387e9287d45faaa52cbcbf8ba314e",
            "424e1d0c54fe4402840523137192cce5",
            "62bdf68f755e4807b99812b96b6c4650",
            "56fbe22c6057474ba3f55e3fb31e2fd4",
            "4600abd3f12d491cb612f40842e318c2",
            "f0ece1f3f7b642e381cd39eee1a1e1ce",
            "49ce647a05cb476c8323e694ef8c75a1",
            "de8b2271f1b94a30a7e59296ad7bf539",
            "397dbd6fbd9f4ed68515dede70f214e9",
            "44dd870901d64dc2b7ff17ba01f6e6e6",
            "11414c12cfbe48f5b0102340253dfc09",
            "72793d5468bf42ca9e9983677319df5e",
            "2be2ab24cfab4a648b5e012213d471da",
            "2acca4a476944bae860d2f75796b0fbf",
            "7a994e13d7ba4d7bb524df3555262f1d",
            "863442250bc34de2ab7eccda0c7db918",
            "0abcbb1984fd4628b2cd26236f3c87e0",
            "5983d50d211c41e1b87331109cff47f4",
            "8d8e4c2a5eb84e8da37553486316325f",
            "2648010541bf4ca8a80f0e4ad123378b",
            "8e3590d0ffad435b8fc2c92ba6ab2893",
            "2b609a5f49e94113a77fc857ea54327d",
            "4fa3ac3fca744de0b903a78250c10b5d",
            "ae95e40995f54213819e09e70ad4a693",
            "1f3c1438399348a1a81561a9874a3f82",
            "8883287da7284aa7bbfd8f46435584a1",
            "e9494a97fb804ce5841b52b47605a01d",
            "f70a94c0b54444a2815ffba1bf45d843",
            "132bc8a9ab3e4773a4da6e28e4ef1710",
            "2de981d3722e4f628607c29ec075057a",
            "43bf5327066f4f3f9f67762619c19251",
            "955b71eeec574d80a5988b00f83092ba",
            "cd117474dbc14966becf2807e56e3f71",
            "e5ac4325c015432197c97dca8f274803",
            "cf87ee99139a4c62bd636d4e1d057951",
            "ef8c8784dbc64a2dbd68df9a074e687a",
            "5a31de18685242a99626f9290bad9124",
            "ad493318bc174cdabfc513c6a545a112",
            "0940aaa94f59404986560650e492d996",
            "70deafbf0d674d129fa1654cda8f352a",
            "bc726e4757f44a12b3994804d5cb48ad",
            "954e2d6182a740a3a23c7148211da2f9",
            "22c6eb7643c74513b0bc4200099e7f61"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48f0be16881e410e8093af6479f41757"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/2.80k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11414c12cfbe48f5b0102340253dfc09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b609a5f49e94113a77fc857ea54327d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd117474dbc14966becf2807e56e3f71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 16.9M/16.9M [00:02<00:00, 6.20MiB/s]\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.98) in first 30s of audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
            "100%|██████████| 360M/360M [00:20<00:00, 18.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.99) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.97) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.99) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.99) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.98) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.97) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.98) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.95) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.91) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.86) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.89) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.90) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.86) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.81) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.98) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.88) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.93) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.98) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.99) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.97) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.94) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.98) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.74) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.96) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.96) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.91) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (1.00) in first 30s of audio...\n",
            "MoviePy - Writing audio in output.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (1.00) in first 30s of audio...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "for i in range(len(paths)):\n",
        "  file_name = paths[i].split('/')[-1].split('.')[0]\n",
        "  shutil.copy(f'{file_name}.txt', '/content/drive/MyDrive/whispers_f')"
      ],
      "metadata": {
        "id": "htFV0c5G0PrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import fitz\n",
        "import whisperx\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "device = \"cuda\"\n",
        "batch_size = 10  # reduce if low on GPU mem\n",
        "compute_type = \"float16\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "# Tolerance to decide whether two images are similar\n",
        "similarity_threshold = 0.95\n",
        "\n",
        "\n",
        "paths = []\n",
        "for file in os.listdir('/content/drive/MyDrive/VID'):\n",
        "  paths.append(os.path.join('/content/drive/MyDrive/VID', file))\n",
        "\n",
        "def are_frames_similar(frame_pair):\n",
        "    frame1, frame2 = frame_pair\n",
        "    # Calculate the structural similarity index (SSI) between two frames\n",
        "    # You can use skimage's structural_similarity function for this\n",
        "    # from skimage.metrics import structural_similarity as compare_ssim\n",
        "    # Return True if they are similar above the given threshold\n",
        "    ssi_index, _ = ssim(frame1, frame2, full=True)\n",
        "    return ssi_index > similarity_threshold\n",
        "\n",
        "\n",
        "for i in range(len(paths)):\n",
        "  file_name = paths[i].split('/')[-1].split('.')[0]\n",
        "  # Previous frame for comparison\n",
        "  prev_frame = None\n",
        "\n",
        "  cap = cv2.VideoCapture(paths[i])\n",
        "  frames = []\n",
        "\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frames.append(gray_frame)\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "  similar_frames = []\n",
        "  frame_pairs = zip(frames[:-1], frames[1:])\n",
        "  for frame_pair in frame_pairs:\n",
        "      similar_frames.append(are_frames_similar(frame_pair))\n",
        "\n",
        "  unique_slides = []\n",
        "\n",
        "  for i, is_similar in enumerate(similar_frames):\n",
        "      if not is_similar:\n",
        "          slide_image = Image.fromarray(frames[i + 1])\n",
        "          unique_slides.append(slide_image)\n",
        "\n",
        "\n",
        "    # Save the aligned segments to a text file\n",
        "  pdf_path = f'{file_name}.pdf'\n",
        "  pdf_doc = fitz.open()\n",
        "\n",
        "  for slide in unique_slides:\n",
        "      img_byte_arr = io.BytesIO()\n",
        "      slide.save(img_byte_arr, format='PNG')\n",
        "      img_byte_arr = img_byte_arr.getvalue()\n",
        "      img = fitz.open(\"png\", img_byte_arr)\n",
        "      rect = img[0].rect\n",
        "      pdfbytes = img.convert_to_pdf()\n",
        "      img.close()\n",
        "      imgPDF = fitz.open(\"pdf\", pdfbytes)\n",
        "      page = pdf_doc.new_page(width=rect.width, height=rect.height)\n",
        "      page.show_pdf_page(rect, imgPDF, 0)\n",
        "\n",
        "  pdf_doc.save(pdf_path)\n",
        "  pdf_doc.close()\n"
      ],
      "metadata": {
        "id": "nrnQ6a624sFx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "for i in range(len(paths)):\n",
        "  file_name = paths[i].split('/')[-1].split('.')[0]\n",
        "  shutil.copy(f'{file_name}.pdf', '/content/drive/MyDrive/pdf_f')"
      ],
      "metadata": {
        "id": "CiZrYrzpg20q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import io\n",
        "\n",
        "import cv2\n",
        "import PyPDF2\n",
        "from google.cloud import vision\n",
        "from moviepy.editor import VideoFileClip\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import os\n",
        "\n",
        "# Initialize the Google Vision API client\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "paths = []\n",
        "for file in os.listdir('/content/drive/MyDrive/pdf_f'):\n",
        "  paths.append(os.path.join('/content/drive/MyDrive/pdf_f', file))\n",
        "\n",
        "\n",
        "def extract_images_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    images = []\n",
        "    for i in range(len(doc)):\n",
        "        for img_index, img in enumerate(doc.get_page_images(i)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Constructing a filename for each image\n",
        "            image_filename = f\"image_page_{i}_img_{img_index}.png\"\n",
        "\n",
        "            with open(image_filename, \"wb\") as image_file:\n",
        "                image_file.write(image_bytes)\n",
        "\n",
        "            images.append(image_filename)\n",
        "    return images\n",
        "\n",
        "# Function to convert PDF pages to images\n",
        "def convert_pdf_to_images(pdf_path):\n",
        "    return convert_from_path(pdf_path)\n",
        "\n",
        "\n",
        "\n",
        "# Function to interpret an image\n",
        "def interpret_image(image_content):\n",
        "    content = io.BytesIO()\n",
        "    image_content.save(content, format='JPEG')\n",
        "    content = content.getvalue()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "    response = client.text_detection(image=image)\n",
        "\n",
        "    return response.text_annotations[0].description if response.text_annotations else \"\"\n",
        "\n",
        "\n",
        "\n",
        "for pdf_path in range(len(paths)):\n",
        "  file_name = paths[pdf_path].split('/')[-1].split('.')[0]\n",
        "  output_file_path = f'{file_name}.txt'\n",
        "  images = convert_pdf_to_images(paths[pdf_path])\n",
        "  with open(output_file_path, 'a') as f:\n",
        "    for image in images:\n",
        "      description = interpret_image(image)\n",
        "      f.write(description)\n",
        "\n",
        "\n",
        "# import base64\n",
        "# import requests\n",
        "\n",
        "# # OpenAI API Key\n",
        "# api_key = \"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\"\n",
        "\n",
        "# # Function to encode the image\n",
        "# def encode_image(image_path):\n",
        "#   with open(image_path, \"rb\") as image_file:\n",
        "#     return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# # Path to your image\n",
        "# image_path = \"/content/image_page_0_img_0 (4).png\"\n",
        "\n",
        "# # Getting the base64 string\n",
        "# base64_image = encode_image(image_path)\n",
        "\n",
        "# headers = {\n",
        "#   \"Content-Type\": \"application/json\",\n",
        "#   \"Authorization\": f\"Bearer {api_key}\"\n",
        "# }\n",
        "\n",
        "# payload = {\n",
        "#   \"model\": \"gpt-4-vision-preview\",\n",
        "#   \"messages\": [\n",
        "#     {\n",
        "#       \"role\": \"user\",\n",
        "#       \"content\": [\n",
        "#         {\n",
        "#           \"type\": \"text\",\n",
        "#           \"text\": \"What’s in this image?\"\n",
        "#         },\n",
        "#         {\n",
        "#           \"type\": \"image_url\",\n",
        "#           \"image_url\": {\n",
        "#             \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "#           }\n",
        "#         }\n",
        "#       ]\n",
        "#     }\n",
        "#   ],\n",
        "#   \"max_tokens\": 300\n",
        "# }\n",
        "\n",
        "# response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "# print(response.json())"
      ],
      "metadata": {
        "id": "AwsEuV7Bud-B",
        "outputId": "5da83978-9525-4b8e-c83d-53cd9575841d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PDFInfoNotInstalledError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LD_LIBRARY_PATH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoppler_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LD_LIBRARY_PATH\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    972\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdfinfo'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1c47c0986248>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0moutput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{file_name}.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_pdf_to_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-1c47c0986248>\u001b[0m in \u001b[0;36mconvert_pdf_to_images\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Function to convert PDF pages to images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_pdf_to_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mpoppler_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoppler_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     page_count = pdfinfo_from_path(\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserpw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mownerpw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoppler_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoppler_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     )[\"Pages\"]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         raise PDFInfoNotInstalledError(\n\u001b[0m\u001b[1;32m    595\u001b[0m             \u001b[0;34m\"Unable to get page count. Is poppler installed and in PATH?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         )\n",
            "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "id": "c-8v4G_B9gpz",
        "outputId": "86b03adf-23ff-4aa6-faad-8bcc9bcda5d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2\n",
            "Err:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2\n",
            "  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/poppler/poppler-utils_22.02.0-2ubuntu0.2_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pdftoppm -v\n"
      ],
      "metadata": {
        "id": "fW2od3GnAwk-",
        "outputId": "71e25b0d-0e4d-4b5a-9d41-055b33f16fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: pdftoppm: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riH2LDXzA1vD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11ff01d46d77470a83cfafa8c93db44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "141d9328877142f6adc5e5a0ffc74164": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "173f3ff51bbd4c1ba7c0a1fd60f0b04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce99cdfc35b24c91adf5cf0fd66232ca",
              "IPY_MODEL_ec16eee49416491c979beaf7da074cec",
              "IPY_MODEL_8631b889a59c4c91afef3f3ebd5cacd3"
            ],
            "layout": "IPY_MODEL_4e09bb956ac4434b9ebe9730bc6e8a13"
          }
        },
        "1b0e15a208bf4285a2bdc5a3b50e7b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f96250ba3b44200b5bb27421b180283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2085ef83e44f758540ffa4bb0c894a",
            "placeholder": "​",
            "style": "IPY_MODEL_9895504404ec4ad4875a0f002f0cb993",
            "value": " 3.09G/3.09G [00:17&lt;00:00, 66.5MB/s]"
          }
        },
        "2122f1f49e00491abc95dcc601ad57c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24d724db6ba440b9a946a78305b591a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29a0ed056a8e41fea4f77928befd2483": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "346b17b11c754422b06e57e9e6865ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4771587fd314a5b8436c4c897fba6ac",
            "placeholder": "​",
            "style": "IPY_MODEL_29a0ed056a8e41fea4f77928befd2483",
            "value": " 2.20M/2.20M [00:00&lt;00:00, 5.37MB/s]"
          }
        },
        "4271b113ac014323b704588abe7879d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be544bcd6674770997cae6949b38039",
            "placeholder": "​",
            "style": "IPY_MODEL_5a9ebd2c967b49d7bf2388d1019a7ddd",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "455124d5d85d433e8ade08327d4a8e71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5a13a361d3488982170cfdec34874c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e09bb956ac4434b9ebe9730bc6e8a13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f675feff8aa41e3bb266e22bb3f560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5089df5f30344f4db6b8686292869fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a9ebd2c967b49d7bf2388d1019a7ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5afd81fba4864e81b44ae277bee2b652": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae704d3efe3b42c8a7487561849e2629",
              "IPY_MODEL_c85611473faf401e87c26b854f5026e5",
              "IPY_MODEL_1f96250ba3b44200b5bb27421b180283"
            ],
            "layout": "IPY_MODEL_455124d5d85d433e8ade08327d4a8e71"
          }
        },
        "6be544bcd6674770997cae6949b38039": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e21d935032f4c158bba0a0e3af5f970": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95266ef860f041e7bf8947635ae87082",
            "max": 2203239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c4b4e545f6941fc8582003fa8c2199c",
            "value": 2203239
          }
        },
        "72ea9e32138e49619e70fe1265b8ceff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4271b113ac014323b704588abe7879d0",
              "IPY_MODEL_6e21d935032f4c158bba0a0e3af5f970",
              "IPY_MODEL_346b17b11c754422b06e57e9e6865ef5"
            ],
            "layout": "IPY_MODEL_9e579e8584ff43a4b4143902e69f7f29"
          }
        },
        "7521b3bd0412471c80a93c9c9397dd1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8631b889a59c4c91afef3f3ebd5cacd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7521b3bd0412471c80a93c9c9397dd1d",
            "placeholder": "​",
            "style": "IPY_MODEL_4f675feff8aa41e3bb266e22bb3f560b",
            "value": " 460k/460k [00:00&lt;00:00, 1.88MB/s]"
          }
        },
        "8c4b4e545f6941fc8582003fa8c2199c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95266ef860f041e7bf8947635ae87082": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9895504404ec4ad4875a0f002f0cb993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99b30b4fd83648319f95bf65ae8354ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e579e8584ff43a4b4143902e69f7f29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae704d3efe3b42c8a7487561849e2629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d5a13a361d3488982170cfdec34874c",
            "placeholder": "​",
            "style": "IPY_MODEL_24d724db6ba440b9a946a78305b591a1",
            "value": "Downloading model.bin: 100%"
          }
        },
        "aedeb29e9dc248d58fbcf723d520179e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f7409b3cf64020a521be2a86ca09c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b682cdb8611f472295dc4e0362ff56ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c85611473faf401e87c26b854f5026e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2122f1f49e00491abc95dcc601ad57c3",
            "max": 3086912962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e70cc0b29e384b809f37bd768efa185d",
            "value": 3086912962
          }
        },
        "ca2085ef83e44f758540ffa4bb0c894a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce99cdfc35b24c91adf5cf0fd66232ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5089df5f30344f4db6b8686292869fe3",
            "placeholder": "​",
            "style": "IPY_MODEL_e8da9a284f6147f4ae619a478b17a6be",
            "value": "Downloading vocabulary.txt: 100%"
          }
        },
        "e05312b9b710438eb3d2f1c2a9a48053": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff9f8b98b92240d2a6edb6688ecfc2c7",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0e15a208bf4285a2bdc5a3b50e7b2b",
            "value": " 2.80k/2.80k [00:00&lt;00:00, 66.9kB/s]"
          }
        },
        "e70cc0b29e384b809f37bd768efa185d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e73e67fe82ec472f976fa945b9777111": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff821f4e722c438a89d118786bc2e55b",
              "IPY_MODEL_f8d438b81f11493ebde058befbfd0e99",
              "IPY_MODEL_e05312b9b710438eb3d2f1c2a9a48053"
            ],
            "layout": "IPY_MODEL_b682cdb8611f472295dc4e0362ff56ca"
          }
        },
        "e8da9a284f6147f4ae619a478b17a6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebbaac86ca94436a9f8079ea05acd975": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec16eee49416491c979beaf7da074cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_141d9328877142f6adc5e5a0ffc74164",
            "max": 459861,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11ff01d46d77470a83cfafa8c93db44d",
            "value": 459861
          }
        },
        "f4771587fd314a5b8436c4c897fba6ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d438b81f11493ebde058befbfd0e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aedeb29e9dc248d58fbcf723d520179e",
            "max": 2796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5f7409b3cf64020a521be2a86ca09c3",
            "value": 2796
          }
        },
        "ff821f4e722c438a89d118786bc2e55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebbaac86ca94436a9f8079ea05acd975",
            "placeholder": "​",
            "style": "IPY_MODEL_99b30b4fd83648319f95bf65ae8354ea",
            "value": "Downloading config.json: 100%"
          }
        },
        "ff9f8b98b92240d2a6edb6688ecfc2c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f0be16881e410e8093af6479f41757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65e387e9287d45faaa52cbcbf8ba314e",
              "IPY_MODEL_424e1d0c54fe4402840523137192cce5",
              "IPY_MODEL_62bdf68f755e4807b99812b96b6c4650"
            ],
            "layout": "IPY_MODEL_56fbe22c6057474ba3f55e3fb31e2fd4"
          }
        },
        "65e387e9287d45faaa52cbcbf8ba314e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4600abd3f12d491cb612f40842e318c2",
            "placeholder": "​",
            "style": "IPY_MODEL_f0ece1f3f7b642e381cd39eee1a1e1ce",
            "value": "Downloading model.bin: 100%"
          }
        },
        "424e1d0c54fe4402840523137192cce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49ce647a05cb476c8323e694ef8c75a1",
            "max": 3086912962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de8b2271f1b94a30a7e59296ad7bf539",
            "value": 3086912962
          }
        },
        "62bdf68f755e4807b99812b96b6c4650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_397dbd6fbd9f4ed68515dede70f214e9",
            "placeholder": "​",
            "style": "IPY_MODEL_44dd870901d64dc2b7ff17ba01f6e6e6",
            "value": " 3.09G/3.09G [00:38&lt;00:00, 83.9MB/s]"
          }
        },
        "56fbe22c6057474ba3f55e3fb31e2fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4600abd3f12d491cb612f40842e318c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ece1f3f7b642e381cd39eee1a1e1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49ce647a05cb476c8323e694ef8c75a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de8b2271f1b94a30a7e59296ad7bf539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "397dbd6fbd9f4ed68515dede70f214e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dd870901d64dc2b7ff17ba01f6e6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11414c12cfbe48f5b0102340253dfc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72793d5468bf42ca9e9983677319df5e",
              "IPY_MODEL_2be2ab24cfab4a648b5e012213d471da",
              "IPY_MODEL_2acca4a476944bae860d2f75796b0fbf"
            ],
            "layout": "IPY_MODEL_7a994e13d7ba4d7bb524df3555262f1d"
          }
        },
        "72793d5468bf42ca9e9983677319df5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863442250bc34de2ab7eccda0c7db918",
            "placeholder": "​",
            "style": "IPY_MODEL_0abcbb1984fd4628b2cd26236f3c87e0",
            "value": "Downloading config.json: 100%"
          }
        },
        "2be2ab24cfab4a648b5e012213d471da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5983d50d211c41e1b87331109cff47f4",
            "max": 2796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d8e4c2a5eb84e8da37553486316325f",
            "value": 2796
          }
        },
        "2acca4a476944bae860d2f75796b0fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2648010541bf4ca8a80f0e4ad123378b",
            "placeholder": "​",
            "style": "IPY_MODEL_8e3590d0ffad435b8fc2c92ba6ab2893",
            "value": " 2.80k/2.80k [00:00&lt;00:00, 70.1kB/s]"
          }
        },
        "7a994e13d7ba4d7bb524df3555262f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "863442250bc34de2ab7eccda0c7db918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abcbb1984fd4628b2cd26236f3c87e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5983d50d211c41e1b87331109cff47f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d8e4c2a5eb84e8da37553486316325f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2648010541bf4ca8a80f0e4ad123378b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e3590d0ffad435b8fc2c92ba6ab2893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b609a5f49e94113a77fc857ea54327d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fa3ac3fca744de0b903a78250c10b5d",
              "IPY_MODEL_ae95e40995f54213819e09e70ad4a693",
              "IPY_MODEL_1f3c1438399348a1a81561a9874a3f82"
            ],
            "layout": "IPY_MODEL_8883287da7284aa7bbfd8f46435584a1"
          }
        },
        "4fa3ac3fca744de0b903a78250c10b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9494a97fb804ce5841b52b47605a01d",
            "placeholder": "​",
            "style": "IPY_MODEL_f70a94c0b54444a2815ffba1bf45d843",
            "value": "Downloading vocabulary.txt: 100%"
          }
        },
        "ae95e40995f54213819e09e70ad4a693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_132bc8a9ab3e4773a4da6e28e4ef1710",
            "max": 459861,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2de981d3722e4f628607c29ec075057a",
            "value": 459861
          }
        },
        "1f3c1438399348a1a81561a9874a3f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43bf5327066f4f3f9f67762619c19251",
            "placeholder": "​",
            "style": "IPY_MODEL_955b71eeec574d80a5988b00f83092ba",
            "value": " 460k/460k [00:00&lt;00:00, 786kB/s]"
          }
        },
        "8883287da7284aa7bbfd8f46435584a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9494a97fb804ce5841b52b47605a01d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f70a94c0b54444a2815ffba1bf45d843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "132bc8a9ab3e4773a4da6e28e4ef1710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2de981d3722e4f628607c29ec075057a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43bf5327066f4f3f9f67762619c19251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "955b71eeec574d80a5988b00f83092ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd117474dbc14966becf2807e56e3f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5ac4325c015432197c97dca8f274803",
              "IPY_MODEL_cf87ee99139a4c62bd636d4e1d057951",
              "IPY_MODEL_ef8c8784dbc64a2dbd68df9a074e687a"
            ],
            "layout": "IPY_MODEL_5a31de18685242a99626f9290bad9124"
          }
        },
        "e5ac4325c015432197c97dca8f274803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad493318bc174cdabfc513c6a545a112",
            "placeholder": "​",
            "style": "IPY_MODEL_0940aaa94f59404986560650e492d996",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "cf87ee99139a4c62bd636d4e1d057951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70deafbf0d674d129fa1654cda8f352a",
            "max": 2203239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc726e4757f44a12b3994804d5cb48ad",
            "value": 2203239
          }
        },
        "ef8c8784dbc64a2dbd68df9a074e687a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954e2d6182a740a3a23c7148211da2f9",
            "placeholder": "​",
            "style": "IPY_MODEL_22c6eb7643c74513b0bc4200099e7f61",
            "value": " 2.20M/2.20M [00:00&lt;00:00, 2.83MB/s]"
          }
        },
        "5a31de18685242a99626f9290bad9124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad493318bc174cdabfc513c6a545a112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0940aaa94f59404986560650e492d996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70deafbf0d674d129fa1654cda8f352a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc726e4757f44a12b3994804d5cb48ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "954e2d6182a740a3a23c7148211da2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22c6eb7643c74513b0bc4200099e7f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}